{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "64OJ5uNRVIy5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "\n",
        "from conplex_dti.model.architectures import SimpleCoembeddingNoSigmoid\n",
        "\n",
        "from conplex_dti.dataset.datamodules import DTIDataModule\n",
        "from conplex_dti.featurizer import get_featurizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u0RmY82aZwyn"
      },
      "outputs": [],
      "source": [
        "# Necessary for TransferCoembedding class\n",
        "# TODO: eventually move TransferCoembedding class to architectures.py\n",
        "class Cosine(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return nn.CosineSimilarity()(x1, x2)\n",
        "\n",
        "DISTANCE_METRICS = {\"Cosine\": Cosine}\n",
        "ACTIVATIONS = {\"ReLU\": nn.ReLU }\n",
        "\n",
        "\n",
        "# Create a new model that has 2 layers of tensors instead of 1.\n",
        "class TransferCoembedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pre_trained_model:SimpleCoembeddingNoSigmoid,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=\"ReLU\",\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO: initialize these all baased on the pre-trained model\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "        self.latent_activation = ACTIVATIONS[latent_activation]\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension),     # [0]\n",
        "            self.latent_activation(),                         # [1]\n",
        "            # ADD AN ADDITIONAL LAYER AND ACTIVATION FUNCTION\n",
        "            nn.Linear(latent_dimension, latent_dimension),    # [2]\n",
        "            self.latent_activation()                          # [3]\n",
        "        )\n",
        "\n",
        "        # initialize layer 0 from pre-trained model:\n",
        "        self.drug_projector[0] = copy.deepcopy(pre_trained_model.drug_projector[0])\n",
        "        # initialize layer 2 randomly\n",
        "        nn.init.xavier_normal_(self.drug_projector[2].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension),\n",
        "            self.latent_activation(),\n",
        "            # ADD AN ADDITIONAL LAYER AND ACTIVATION FUNCTION\n",
        "            nn.Linear(latent_dimension, latent_dimension),\n",
        "            self.latent_activation()\n",
        "        )\n",
        "\n",
        "        # initialize layer 0 from pre-trained model:\n",
        "        self.target_projector[0] = copy.deepcopy(pre_trained_model.target_projector[0])\n",
        "        # initialize layer 2 randomly\n",
        "        nn.init.xavier_normal_(self.target_projector[2].weight)\n",
        "\n",
        "        # freeze the first layers (neural net ) of the target and drug projectors\n",
        "        for idx, param in enumerate(self.parameters()):\n",
        "            if idx == 0 or idx == 1 or idx == 4 or idx == 5:\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "        if self.do_classify: # if True:\n",
        "            self.distance_metric = latent_distance # \"Cosine\"\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]() # gives it the Cosine activator function that was written\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify: # if True:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        return inner_prod.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bcthvn10mFQr"
      },
      "outputs": [],
      "source": [
        "# TODO: want to try learning / training on three different models\n",
        "\n",
        "# 1. TransferCoembedding model with two unfrozen layers\n",
        "# 2. original model with only RELU layer unfrozen\n",
        "# 3. original model with lower learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J9Pg3cj10xUg"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Drug and target featurizers already exist\n",
            "Morgan: 100%|██████████| 4/4 [00:00<00:00, 2019.40it/s]\n",
            "ProtBert: 100%|██████████| 2/2 [00:00<00:00, 4140.48it/s]\n"
          ]
        }
      ],
      "source": [
        "# recreate lines 309 -> ~322 from train.py\n",
        "\n",
        "# load the featurizers \n",
        "drug_featurizer = get_featurizer(\"MorganFeaturizer\")\n",
        "# load from pre-trained prot-bert model\n",
        "target_featurizer = get_featurizer(\"ProtBertFeaturizer\")\n",
        "\n",
        "\n",
        "# load data into a drug-target interaction datamodule (combo of dataset/dataloader)\n",
        "datamodule = DTIDataModule(data_dir = \"./data/\", drug_featurizer=drug_featurizer, target_featurizer=target_featurizer, batch_size=2)\n",
        "datamodule.prepare_data()\n",
        "datamodule.setup()\n",
        "\n",
        "# from the datamodules, get dataloaders to use for training, testing, and validation\n",
        "training_generator = datamodule.train_dataloader()\n",
        "validation_generator = datamodule.val_dataloader()\n",
        "testing_generator = datamodule.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch index: 0\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 1\n",
            "\n",
            "batch index: 1\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 1\n",
            "\n",
            "batch index: 2\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 0\n",
            "\n",
            "batch index: 3\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, batch in enumerate(training_generator):\n",
        "    print(f\"batch index: {batch_idx}\")\n",
        "    print(f\"batch size: {len(batch[0])}\")\n",
        "\n",
        "    print(f\"MF tensor: {len(batch[0])}x{len(batch[0][0])}\")\n",
        "    print(f\"PB tensor: {len(batch[1])}x{len(batch[1][0])}\")\n",
        "    print(f\"binding yes/no label: {batch[2][0]}\")\n",
        "    print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the (new) model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load previous model from state dict!\n",
        "from conplex_dti.model.architectures import SimpleCoembeddingNoSigmoid\n",
        "\n",
        "pre_trained_model = SimpleCoembeddingNoSigmoid(2048, 1024, 1024)  # TODO: drug_featurizer.shape, target_featurizer.shape 2048, 1024, 1024\n",
        "pre_trained_model.load_state_dict(torch.load(\"../pre_trained_model/models/ConPLex_v1_BindingDB.pt\", map_location=\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from conplex_dti.model.architectures import TransferCoembedding # TODO: move from notebook to architectures file \n",
        "model = TransferCoembedding(pre_trained_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for param in model.parameters():\n",
        "#     print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizer:\n",
        "LEARNING_RATE = 0.01 # not same as sam, larger\n",
        "RESET_AFTER_EPOCHS = 10 # aka lr_t0 from config file TODO: play with this, currently same as sam\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    opt, T_0=RESET_AFTER_EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keep track of best model:\n",
        "max_metric = 0\n",
        "model_max = copy.deepcopy(model) # currently last (2) layers are random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# line 413 -> 423, NOT doing contrastive learning\n",
        "# loss function:\n",
        "loss_fct = torch.nn.BCELoss() # TODO: what is BCE loss\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "# TODO: UNDERSTAND THIS\n",
        "val_metrics = {\n",
        "    \"val/aupr\": torchmetrics.AveragePrecision,\n",
        "    \"val/auroc\": torchmetrics.AUROC,\n",
        "}\n",
        "\n",
        "test_metrics = {\n",
        "    \"test/aupr\": torchmetrics.AveragePrecision,\n",
        "    \"test/auroc\": torchmetrics.AUROC,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning Training\n"
          ]
        }
      ],
      "source": [
        "# TODO: eventually do do weights and biases logging\n",
        "# TODO: eventually do logg\n",
        "\n",
        "print(f\"Beginning Training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS FOR TRAINING:\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def step(model, batch, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    drug, target, label = batch  # target is (D + N_pool)\n",
        "    pred = model(drug.to(device), target.to(device))\n",
        "    label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
        "    return pred, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS FOR TESTING:\n",
        "def test(model, data_generator, metrics, device=None, classify=True):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    metric_dict = {}\n",
        "\n",
        "    for k, met_class in metrics.items():\n",
        "        if classify:\n",
        "            met_instance = met_class(task=\"binary\")\n",
        "        else:\n",
        "            met_instance = met_class()\n",
        "        met_instance.to(device)\n",
        "        met_instance.reset()\n",
        "        metric_dict[k] = met_instance\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, batch in enumerate(data_generator):\n",
        "        pred, label = step(model, batch, device)\n",
        "        if classify:\n",
        "            label = label.int()\n",
        "        else:\n",
        "            label = label.float()\n",
        "\n",
        "        for _, met_instance in metric_dict.items():\n",
        "            met_instance(pred, label)\n",
        "\n",
        "    results = {}\n",
        "    for k, met_instance in metric_dict.items():\n",
        "        res = met_instance.compute()\n",
        "        results[k] = res\n",
        "\n",
        "    for met_instance in metric_dict.values():\n",
        "        met_instance.to(\"cpu\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 3 # TODO: play with this\n",
        "VALIDATE_AFTER_EPOCHS = 1 # TODO: play with this\n",
        "CLASSIFY = True # 309\n",
        "WATCH_METRIC = \"val/aupr\" # 310\n",
        "\n",
        "\n",
        "import torch\n",
        "SAVE_DIRECTORY = \"./saved_models/\" # TODO: eventually change this\n",
        "RUN_ID = \"test_run\" # TODO: eventually change this\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainign epoch is 1\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n",
            "batch number 4\n",
            "loss was 1.748757. Updating learning rate to 0.009755\n",
            "Validation AUPR 0.892857 > previous max 0.000000\n",
            "Saving checkpoint model to saved_models/test_run_best_model_epoch00.pt\n",
            "Validation at Epoch 1\n",
            "val/aupr: 0.8928571343421936\n",
            "val/auroc: 0.8125\n",
            "Trainign epoch is 2\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n",
            "batch number 4\n",
            "loss was 0.466989. Updating learning rate to 0.009045\n",
            "Validation AUPR 1.000000 > previous max 0.892857\n",
            "Saving checkpoint model to saved_models/test_run_best_model_epoch01.pt\n",
            "Validation at Epoch 2\n",
            "val/aupr: 1.0\n",
            "val/auroc: 1.0\n",
            "Trainign epoch is 3\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n",
            "batch number 4\n",
            "loss was 0.344101. Updating learning rate to 0.007939\n",
            "Validation at Epoch 3\n",
            "val/aupr: 1.0\n",
            "val/auroc: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epo in range(NUM_EPOCHS): # TODO: eventually do tqdm (progress bar)\n",
        "\n",
        "    print(f\"Trainign epoch is {epo + 1}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for i, batch in enumerate(training_generator):\n",
        "        print(f\"batch number {i + 1}\")\n",
        "        pred, label = step(model, batch, DEVICE)  # batch is (2048, 1024, 1)\n",
        "        loss = loss_fct(pred, label)\n",
        "        \n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        # break\n",
        "    lr_scheduler.step()\n",
        "    print(f\"loss was {loss.cpu().detach().numpy():8f}. Updating learning rate to {lr_scheduler.get_lr()[0]:8f}\")\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    if epo % VALIDATE_AFTER_EPOCHS == 0:\n",
        "        with torch.set_grad_enabled(False):\n",
        "            val_results = test(\n",
        "                model = model,\n",
        "                data_generator = validation_generator,\n",
        "                metrics = val_metrics,\n",
        "                device = DEVICE,\n",
        "                classify = CLASSIFY,\n",
        "            )\n",
        "            val_results[\"epoch\"] = epo \n",
        "            \n",
        "            if val_results[WATCH_METRIC] > max_metric:\n",
        "                print(f\"Validation AUPR {val_results[WATCH_METRIC]:8f} > previous max {max_metric:8f}\")\n",
        "                model_max = copy.deepcopy(model)\n",
        "                max_metric = val_results[WATCH_METRIC]\n",
        "                model_save_path = Path(\n",
        "                    f\"{SAVE_DIRECTORY}/{RUN_ID}_best_model_epoch{epo:02}.pt\"\n",
        "                )\n",
        "                torch.save(\n",
        "                    model_max.state_dict(),\n",
        "                    model_save_path,\n",
        "                )\n",
        "                print(f\"Saving checkpoint model to {model_save_path}\")\n",
        "                \n",
        "            print(f\"Validation at Epoch {epo + 1}\")\n",
        "            for k, v in val_results.items():\n",
        "                if not k.startswith(\"_\") and not k.startswith(\"epoch\"):\n",
        "                    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning testing\n",
            "Final Testing\n",
            "test/aupr: 1.0\n",
            "test/auroc: 1.0\n",
            "Saving final model to saved_models/test_run_best_model.pt\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "print(\"Beginning testing\")\n",
        "try:\n",
        "    with torch.set_grad_enabled(False):\n",
        "        model_max = model_max.eval()\n",
        "\n",
        "        test_results = test(\n",
        "            model = model_max,\n",
        "            data_generator = testing_generator,\n",
        "            metrics = test_metrics,\n",
        "            device = DEVICE,\n",
        "            classify = CLASSIFY,\n",
        "        )\n",
        "        \n",
        "        test_results[\"epoch\"] = epo + 1\n",
        "\n",
        "        print(\"Final Testing\")\n",
        "        for k, v in test_results.items():\n",
        "            if not k.startswith(\"_\") and not k.startswith(\"epoch\"):\n",
        "                print(f\"{k}: {v}\")\n",
        "\n",
        "        model_save_path = Path(f\"{SAVE_DIRECTORY}/{RUN_ID}_best_model.pt\")\n",
        "        torch.save(\n",
        "            model_max.state_dict(),\n",
        "            model_save_path,\n",
        "        )\n",
        "        print(f\"Saving final model to {model_save_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Testing failed with exception {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
