{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "64OJ5uNRVIy5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from conplex_dti.featurizer import (\n",
        "    MorganFeaturizer,\n",
        "    ProtBertFeaturizer,\n",
        ")\n",
        "\n",
        "from conplex_dti.model.architectures import SimpleCoembeddingNoSigmoid\n",
        "from conplex_dti.model.architectures import SimpleCoembedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: eventually move TransferCoembedding class to architectures.py\n",
        "class Cosine(nn.Module):\n",
        "    def forward(self, x1, x2):\n",
        "        return nn.CosineSimilarity()(x1, x2)\n",
        "\n",
        "\n",
        "DISTANCE_METRICS = {\n",
        "    \"Cosine\": Cosine,\n",
        "}\n",
        "ACTIVATIONS = {\"ReLU\": nn.ReLU }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u0RmY82aZwyn"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# Create a new model that has 2 layers of tensors instead of 1.\n",
        "class TransferCoembedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pre_trained_model:SimpleCoembedding,\n",
        "        drug_shape=2048,\n",
        "        target_shape=1024,\n",
        "        latent_dimension=1024,\n",
        "        latent_activation=\"ReLU\",\n",
        "        latent_distance=\"Cosine\",\n",
        "        classify=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO: initialize these all baased on the pre-trained model\n",
        "        self.drug_shape = drug_shape\n",
        "        self.target_shape = target_shape\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.do_classify = classify\n",
        "        self.latent_activation = ACTIVATIONS[latent_activation]\n",
        "\n",
        "        self.drug_projector = nn.Sequential(\n",
        "            nn.Linear(self.drug_shape, latent_dimension),     # [0]\n",
        "            self.latent_activation(),                         # [1]\n",
        "            # ADD AN ADDITIONAL LAYER AND ACTIVATION FUNCTION\n",
        "            nn.Linear(latent_dimension, latent_dimension),    # [2]\n",
        "            self.latent_activation()                          # [3]\n",
        "        )\n",
        "\n",
        "        # initialize layer 0 from pre-trained model:\n",
        "        self.drug_projector[0] = copy.deepcopy(pre_trained_model.drug_projector[0])\n",
        "        # initialize layer 2 randomly\n",
        "        nn.init.xavier_normal_(self.drug_projector[2].weight)\n",
        "\n",
        "        self.target_projector = nn.Sequential(\n",
        "            nn.Linear(self.target_shape, latent_dimension),\n",
        "            self.latent_activation(),\n",
        "            # ADD AN ADDITIONAL LAYER AND ACTIVATION FUNCTION\n",
        "            nn.Linear(latent_dimension, latent_dimension),\n",
        "            self.latent_activation()\n",
        "        )\n",
        "\n",
        "        # initialize layer 0 from pre-trained model:\n",
        "        self.target_projector[0] = copy.deepcopy(pre_trained_model.target_projector[0])\n",
        "        # initialize layer 2 randomly\n",
        "        nn.init.xavier_normal_(self.target_projector[2].weight)\n",
        "\n",
        "        # freeze the first layers (neural net ) of the target and drug projectors\n",
        "        for idx, param in enumerate(self.parameters()):\n",
        "            if idx == 0 or idx == 1 or idx == 4 or idx == 5:\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "        if self.do_classify: # if True:\n",
        "            self.distance_metric = latent_distance # \"Cosine\"\n",
        "            self.activator = DISTANCE_METRICS[self.distance_metric]() # gives it the Cosine activator function that was written\n",
        "\n",
        "    def forward(self, drug, target):\n",
        "        if self.do_classify: # if True:\n",
        "            return self.classify(drug, target)\n",
        "        else:\n",
        "            return self.regress(drug, target)\n",
        "\n",
        "    def classify(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        distance = self.activator(drug_projection, target_projection)\n",
        "        return distance.squeeze()\n",
        "\n",
        "    def regress(self, drug, target):\n",
        "        drug_projection = self.drug_projector(drug)\n",
        "        target_projection = self.target_projector(target)\n",
        "\n",
        "        inner_prod = torch.bmm(\n",
        "            drug_projection.view(-1, 1, self.latent_dimension),\n",
        "            target_projection.view(-1, self.latent_dimension, 1),\n",
        "        ).squeeze()\n",
        "        return inner_prod.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KRQTM4GmbIXY"
      },
      "source": [
        "there are 2 ways to apply transfer learning.\n",
        "\n",
        "1. Give the model the new images and allow it to train with a lower learning rate\n",
        "2. take the given model and freeze all layers except for the last, output layer, then retrain\n",
        "\n",
        "we are going to adopt method 2., since our model has only one layer, we are adding a 2nd layer that is randomly initialized"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1uBLuSKlcibF"
      },
      "source": [
        "NOTES:\n",
        "- will get bitter data from paper and give binding vs not binding for bitter molecules and receptors so we can do binary training\n",
        "- still unsure on how we will do sweet molecules / address the sweet receptor\n",
        "- other people are also interested in this, so we could potentially contribute it back to sam\n",
        "- we could also apply method 1 as well"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MORE NOTES ON THE MODEL(S):\n",
        "- currently we have added a new network layer, and a new output layer!\n",
        "MAYBE we just want to train the RELU (output) layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bcthvn10mFQr"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "# - use constrative dataset as input for two models: one that is the original with a lower learning rate,\n",
        "#   the ohter model that is our creation\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a DTIDataModule (from conplex_dti/dataset/datamodules.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "upLSKOGy0ZUF"
      },
      "outputs": [],
      "source": [
        "# import pytorch_lightning as pl\n",
        "# from conplex_dti.featurizer import Featurizer\n",
        "# import typing as T\n",
        "# from pathlib import Path\n",
        "\n",
        "\n",
        "# def drug_target_collate_fn(args: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
        "#     \"\"\"\n",
        "#     Collate function for PyTorch data loader -- turn a batch of triplets into a triplet of batches\n",
        "\n",
        "#     If target embeddings are not all the same length, it will zero pad them\n",
        "#     This is to account for differences in length from FoldSeek embeddings\n",
        "\n",
        "#     :param args: Batch of training samples with molecule, protein, and affinity\n",
        "#     :type args: Iterable[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]\n",
        "#     :return: Create a batch of examples\n",
        "#     :rtype: T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "#     \"\"\"\n",
        "#     d_emb = [a[0] for a in args]\n",
        "#     t_emb = [a[1] for a in args]\n",
        "#     labs = [a[2] for a in args]\n",
        "\n",
        "#     drugs = torch.stack(d_emb, 0)\n",
        "#     targets = pad_sequence(t_emb, batch_first=True, padding_value=FOLDSEEK_MISSING_IDX)\n",
        "#     labels = torch.stack(labs, 0)\n",
        "\n",
        "#     return drugs, targets, labels\n",
        "\n",
        "\n",
        "# class DTIDataModule(pl.LightningDataModule):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         data_dir: str,\n",
        "#         drug_featurizer: Featurizer,\n",
        "#         target_featurizer: Featurizer,\n",
        "#         device: torch.device = torch.device(\"cpu\"),\n",
        "#         batch_size: int = 32,\n",
        "#         shuffle: bool = True,\n",
        "#         num_workers: int = 0,\n",
        "#         header=0,\n",
        "#         index_col=0,\n",
        "#         sep=\",\",\n",
        "#     ):\n",
        "#         self._loader_kwargs = {\n",
        "#             \"batch_size\": batch_size,\n",
        "#             \"shuffle\": shuffle,\n",
        "#             \"num_workers\": num_workers,\n",
        "#             \"collate_fn\": drug_target_collate_fn,\n",
        "#         }\n",
        "\n",
        "#         self._csv_kwargs = {\n",
        "#             \"header\": header,\n",
        "#             \"index_col\": index_col,\n",
        "#             \"sep\": sep,\n",
        "#         }\n",
        "\n",
        "#         self._device = device\n",
        "\n",
        "#         self._data_dir = Path(data_dir)\n",
        "#         self._train_path = Path(\"train.csv\")\n",
        "#         self._val_path = Path(\"val.csv\")\n",
        "#         self._test_path = Path(\"test.csv\")\n",
        "\n",
        "#         self._drug_column = \"SMILES\"\n",
        "#         self._target_column = \"Target Sequence\"\n",
        "#         self._label_column = \"Label\"\n",
        "\n",
        "#         self.drug_featurizer = drug_featurizer\n",
        "#         self.target_featurizer = target_featurizer\n",
        "\n",
        "#     def prepare_data(self):\n",
        "#         if self.drug_featurizer.path.exists() and self.target_featurizer.path.exists():\n",
        "#             logg.warning(\"Drug and target featurizers already exist\")\n",
        "#             return\n",
        "\n",
        "#         df_train = pd.read_csv(self._data_dir / self._train_path, **self._csv_kwargs)\n",
        "\n",
        "#         df_val = pd.read_csv(self._data_dir / self._val_path, **self._csv_kwargs)\n",
        "\n",
        "#         df_test = pd.read_csv(self._data_dir / self._test_path, **self._csv_kwargs)\n",
        "\n",
        "#         dataframes = [df_train, df_val, df_test]\n",
        "\n",
        "#         all_drugs = pd.concat([i[self._drug_column] for i in dataframes]).unique()\n",
        "\n",
        "\n",
        "#         all_targets = pd.concat([i[self._target_column] for i in dataframes]).unique()\n",
        "\n",
        "#         if self._device.type == \"cuda\":\n",
        "#             self.drug_featurizer.cuda(self._device)\n",
        "#             self.target_featurizer.cuda(self._device)\n",
        "\n",
        "#         if not self.drug_featurizer.path.exists():\n",
        "#             self.drug_featurizer.write_to_disk(all_drugs)\n",
        "\n",
        "#         if not self.target_featurizer.path.exists():\n",
        "#             self.target_featurizer.write_to_disk(all_targets)\n",
        "\n",
        "#         self.drug_featurizer.cpu()\n",
        "#         self.target_featurizer.cpu()\n",
        "\n",
        "#     def setup(self, stage: T.Optional[str] = None):\n",
        "#         self.df_train = pd.read_csv(\n",
        "#             self._data_dir / self._train_path, **self._csv_kwargs\n",
        "#         )\n",
        "\n",
        "#         self.df_val = pd.read_csv(self._data_dir / self._val_path, **self._csv_kwargs)\n",
        "\n",
        "#         self.df_test = pd.read_csv(self._data_dir / self._test_path, **self._csv_kwargs)\n",
        "\n",
        "#         self._dataframes = [self.df_train, self.df_val, self.df_test]\n",
        "\n",
        "#         all_drugs = pd.concat([i[self._drug_column] for i in self._dataframes]).unique()\n",
        "#         all_targets = pd.concat(\n",
        "#             [i[self._target_column] for i in self._dataframes]\n",
        "#         ).unique()\n",
        "\n",
        "#         if self._device.type == \"cuda\":\n",
        "#             self.drug_featurizer.cuda(self._device)\n",
        "#             self.target_featurizer.cuda(self._device)\n",
        "\n",
        "#         self.drug_featurizer.preload(all_drugs)\n",
        "#         self.drug_featurizer.cpu()\n",
        "\n",
        "#         self.target_featurizer.preload(all_targets)\n",
        "#         self.target_featurizer.cpu()\n",
        "\n",
        "#         if stage == \"fit\" or stage is None:\n",
        "#             self.data_train = BinaryDataset(\n",
        "#                 self.df_train[self._drug_column],\n",
        "#                 self.df_train[self._target_column],\n",
        "#                 self.df_train[self._label_column],\n",
        "#                 self.drug_featurizer,\n",
        "#                 self.target_featurizer,\n",
        "#             )\n",
        "\n",
        "#             self.data_val = BinaryDataset(\n",
        "#                 self.df_val[self._drug_column],\n",
        "#                 self.df_val[self._target_column],\n",
        "#                 self.df_val[self._label_column],\n",
        "#                 self.drug_featurizer,\n",
        "#                 self.target_featurizer,\n",
        "#             )\n",
        "\n",
        "#         if stage == \"test\" or stage is None:\n",
        "#             self.data_test = BinaryDataset(\n",
        "#                 self.df_test[self._drug_column],\n",
        "#                 self.df_test[self._target_column],\n",
        "#                 self.df_test[self._label_column],\n",
        "#                 self.drug_featurizer,\n",
        "#                 self.target_featurizer,\n",
        "#             )\n",
        "\n",
        "#     def train_dataloader(self):\n",
        "#         return DataLoader(self.data_train, **self._loader_kwargs)\n",
        "\n",
        "#     def val_dataloader(self):\n",
        "#         return DataLoader(self.data_val, **self._loader_kwargs)\n",
        "\n",
        "#     def test_dataloader(self):\n",
        "#         return DataLoader(self.data_test, **self._loader_kwargs)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drug_featurizer = MorganFeaturizer()\n",
        "# target_featurizer = ProtBertFeaturizer()\n",
        "# our_data = DTIDataModule(data_dir = \"./data/\", drug_featurizer=drug_featurizer, target_featurizer=target_featurizer, batch_size = 4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- since the sate we are working "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J9Pg3cj10xUg"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# task is biosnap SO use DTIDataModule\n",
        "# decided on biosnap as task bc data was in most similar format to ours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Drug and target featurizers already exist\n",
            "Morgan: 100%|██████████| 4/4 [00:00<00:00, 1385.06it/s]\n",
            "ProtBert: 100%|██████████| 2/2 [00:00<00:00, 4739.33it/s]\n"
          ]
        }
      ],
      "source": [
        "# recreate lines 309 -> ~322 from train.py\n",
        "\n",
        "from conplex_dti.dataset.datamodules import DTIDataModule\n",
        "from conplex_dti.featurizer import get_featurizer\n",
        "drug_featurizer = get_featurizer(\"MorganFeaturizer\")\n",
        "target_featurizer = get_featurizer(\"ProtBertFeaturizer\")\n",
        "\n",
        "\n",
        "datamodule = DTIDataModule(data_dir = \"./data/\", drug_featurizer=drug_featurizer, target_featurizer=target_featurizer, batch_size=2)\n",
        "\n",
        "datamodule.prepare_data()\n",
        "datamodule.setup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOW, get dataloaders\n",
        "\n",
        "training_generator = datamodule.train_dataloader()\n",
        "validation_generator = datamodule.val_dataloader()\n",
        "testing_generator = datamodule.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch index: 0\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 0\n",
            "\n",
            "batch index: 1\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 0\n",
            "\n",
            "batch index: 2\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 1\n",
            "\n",
            "batch index: 3\n",
            "batch size: 2\n",
            "MF tensor: 2x2048\n",
            "PB tensor: 2x1024\n",
            "binding yes/no label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, batch in enumerate(training_generator):\n",
        "    print(f\"batch index: {batch_idx}\")\n",
        "    print(f\"batch size: {len(batch[0])}\")\n",
        "\n",
        "    print(f\"MF tensor: {len(batch[0])}x{len(batch[0][0])}\")\n",
        "    print(f\"PB tensor: {len(batch[1])}x{len(batch[1][0])}\")\n",
        "    print(f\"binding yes/no label: {batch[2][0]}\")\n",
        "    print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the (new) model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load previous model from state dict!\n",
        "from conplex_dti.model.architectures import SimpleCoembeddingNoSigmoid\n",
        "\n",
        "pre_trained_model = SimpleCoembeddingNoSigmoid(2048, 1024, 1024)  # TODO: drug_featurizer.shape, target_featurizer.shape 2048, 1024, 1024\n",
        "pre_trained_model.load_state_dict(torch.load(\"../pre_trained_model/models/ConPLex_v1_BindingDB.pt\", map_location=\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from conplex_dti.model.architectures import TransferCoembedding # TODO: move from notebook to architectures file \n",
        "model = TransferCoembedding(pre_trained_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for param in model.parameters():\n",
        "#     print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizer:\n",
        "LEARNING_RATE = 0.01 # not same as sam, larger\n",
        "RESET_AFTER_EPOCHS = 10 # aka lr_t0 from config file TODO: play with this, currently same as sam\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    opt, T_0=RESET_AFTER_EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keep track of best model:\n",
        "max_metric = 0\n",
        "model_max = copy.deepcopy(model) # currently last (2) layers are random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# line 413 -> 423, NOT doing contrastive learning\n",
        "# loss function:\n",
        "loss_fct = torch.nn.BCELoss() # TODO: what is BCE loss\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "# TODO: UNDERSTAND THIS\n",
        "val_metrics = {\n",
        "    \"val/aupr\": torchmetrics.AveragePrecision,\n",
        "    \"val/auroc\": torchmetrics.AUROC,\n",
        "}\n",
        "\n",
        "test_metrics = {\n",
        "    \"test/aupr\": torchmetrics.AveragePrecision,\n",
        "    \"test/auroc\": torchmetrics.AUROC,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning Training\n"
          ]
        }
      ],
      "source": [
        "# TODO: eventually do do weights and biases logging\n",
        "# TODO: eventually do logg\n",
        "\n",
        "print(f\"Beginning Training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS FOR TRAINING:\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def step(model, batch, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    drug, target, label = batch  # target is (D + N_pool)\n",
        "    pred = model(drug.to(device), target.to(device))\n",
        "    label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
        "    return pred, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS FOR TESTING:\n",
        "def test(model, data_generator, metrics, device=None, classify=True):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    metric_dict = {}\n",
        "\n",
        "    for k, met_class in metrics.items():\n",
        "        if classify:\n",
        "            met_instance = met_class(task=\"binary\")\n",
        "        else:\n",
        "            met_instance = met_class()\n",
        "        met_instance.to(device)\n",
        "        met_instance.reset()\n",
        "        metric_dict[k] = met_instance\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, batch in enumerate(data_generator):\n",
        "        pred, label = step(model, batch, device)\n",
        "        if classify:\n",
        "            label = label.int()\n",
        "        else:\n",
        "            label = label.float()\n",
        "\n",
        "        for _, met_instance in metric_dict.items():\n",
        "            met_instance(pred, label)\n",
        "\n",
        "    results = {}\n",
        "    for k, met_instance in metric_dict.items():\n",
        "        res = met_instance.compute()\n",
        "        results[k] = res\n",
        "\n",
        "    for met_instance in metric_dict.values():\n",
        "        met_instance.to(\"cpu\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 3 # TODO: play with this\n",
        "VALIDATE_AFTER_EPOCHS = 1 # TODO: play with this\n",
        "CLASSIFY = True # 309\n",
        "WATCH_METRIC = \"val/aupr\" # 310\n",
        "\n",
        "\n",
        "import torch\n",
        "SAVE_DIRECTORY = \"./saved_models/\" # TODO: eventually change this\n",
        "RUN_ID = \"test\" # TODO: eventually change this\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainign epoch is 1\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number 4\n",
            "loss was 0.677217. Updating learning rate to 0.009755\n",
            "Validation AUPR 1.000000 > previous max 0.000000\n",
            "Saving checkpoint model to saved_models/test_best_model_epoch00.pt\n",
            "Validation at Epoch 1\n",
            "val/aupr: 1.0\n",
            "val/auroc: 1.0\n",
            "Trainign epoch is 2\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n",
            "batch number 4\n",
            "loss was 0.573348. Updating learning rate to 0.009045\n",
            "Validation at Epoch 2\n",
            "val/aupr: 1.0\n",
            "val/auroc: 1.0\n",
            "Trainign epoch is 3\n",
            "batch number 1\n",
            "batch number 2\n",
            "batch number 3\n",
            "batch number 4\n",
            "loss was 0.451817. Updating learning rate to 0.007939\n",
            "Validation at Epoch 3\n",
            "val/aupr: 1.0\n",
            "val/auroc: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epo in range(NUM_EPOCHS): # TODO: eventually do tqdm (progress bar)\n",
        "\n",
        "    print(f\"Trainign epoch is {epo + 1}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for i, batch in enumerate(training_generator):\n",
        "        print(f\"batch number {i + 1}\")\n",
        "        pred, label = step(model, batch, device)  # batch is (2048, 1024, 1)\n",
        "        loss = loss_fct(pred, label)\n",
        "        \n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        # break\n",
        "    lr_scheduler.step()\n",
        "    print(f\"loss was {loss.cpu().detach().numpy():8f}. Updating learning rate to {lr_scheduler.get_lr()[0]:8f}\")\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    if epo % VALIDATE_AFTER_EPOCHS == 0:\n",
        "        with torch.set_grad_enabled(False):\n",
        "            val_results = test(\n",
        "                model = model,\n",
        "                data_generator = validation_generator,\n",
        "                metrics = val_metrics,\n",
        "                device = device,\n",
        "                classify = CLASSIFY,\n",
        "            )\n",
        "            val_results[\"epoch\"] = epo \n",
        "            \n",
        "            if val_results[WATCH_METRIC] > max_metric:\n",
        "                print(f\"Validation AUPR {val_results[WATCH_METRIC]:8f} > previous max {max_metric:8f}\")\n",
        "                model_max = copy.deepcopy(model)\n",
        "                max_metric = val_results[WATCH_METRIC]\n",
        "                model_save_path = Path(\n",
        "                    f\"{SAVE_DIRECTORY}/{RUN_ID}_best_model_epoch{epo:02}.pt\"\n",
        "                )\n",
        "                torch.save(\n",
        "                    model_max.state_dict(),\n",
        "                    model_save_path,\n",
        "                )\n",
        "                print(f\"Saving checkpoint model to {model_save_path}\")\n",
        "                \n",
        "            print(f\"Validation at Epoch {epo + 1}\")\n",
        "            for k, v in val_results.items():\n",
        "                if not k.startswith(\"_\") and not k.startswith(\"epoch\"):\n",
        "                    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'val/aupr': tensor(1.), 'val/auroc': tensor(1.), 'epoch': 2}\n"
          ]
        }
      ],
      "source": [
        "print(val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
